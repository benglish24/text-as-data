{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-2 Grammars & Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved normalization function\n",
    "# Removes all punctuation except periods (for sentences)\n",
    "# Lowercases all words and tokenizes\n",
    "\n",
    "def tknize (a_string):\n",
    "    # Handle all the string operations at one time\n",
    "    clean = re.sub('[^a-zA-Z \\.]', ' ', a_string).lower()\n",
    "    # Create the list of sub-strings (tokens) and return it\n",
    "    return nltk.tokenize.word_tokenize(clean)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading, Cleaning, Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Off there to the right -- somewhere -- is a large\n",
      "\"ACT I\"\n",
      "\"SCENE I. London. The palace.\"\n",
      "\"Enter KING\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "\n",
    "with open(\"../data/mdg.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            mdg = f.read()\n",
    "\n",
    "with open(\"../data/shakespeare.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            shakespeare = f.read()\n",
    "\n",
    "print(mdg[0:50])\n",
    "print(shakespeare[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Off there to the right -- somewhere -- is a large\n"
     ]
    }
   ],
   "source": [
    "print(mdg[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['off',\n",
       " 'there',\n",
       " 'to',\n",
       " 'the',\n",
       " 'right',\n",
       " 'somewhere',\n",
       " 'is',\n",
       " 'a',\n",
       " 'large',\n",
       " 'island',\n",
       " 'said',\n",
       " 'whitney',\n",
       " '.',\n",
       " 'it',\n",
       " 's',\n",
       " 'rather',\n",
       " 'a',\n",
       " 'mystery',\n",
       " 'what',\n",
       " 'island']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Break them into a list of tokens\n",
    "mdg_ = tknize(mdg)\n",
    "mdg_[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning can mean simply filtering stop tokens, because we want to include those angle brackets, as well as normalizing and possibly (possibly) lemmatizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "See the [list of PoS tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) on the UPenn website for details on tags. For a complete list, run the following: `nltk.help.upenn_tagset()` -- please make sure you download the tag set first, `nltk.download(\"tagsets\")`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rather', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('mystery', 'NN'),\n",
       " ('what', 'WP'),\n",
       " ('island', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('rainsford', 'JJ'),\n",
       " ('asked', 'VBD'),\n",
       " ('.', '.'),\n",
       " ('the', 'DT'),\n",
       " ('old', 'JJ'),\n",
       " ('charts', 'NNS'),\n",
       " ('call', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('ship', 'JJ'),\n",
       " ('trap', 'JJ'),\n",
       " ('island', 'NN'),\n",
       " ('whitney', 'NN'),\n",
       " ('replied', 'VBN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mdg_ is our cleaned list of tokens which keeps only periods.\n",
    "mdg_tagged = nltk.pos_tag(mdg_)\n",
    "mdg_tagged[15:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "off there to the right somewhere is a large island said whitney . it s rather a mystery what island is it rainsford asked . the old charts call it ship trap island whitney replied . a suggestive name isn t it sailors have a curious dread of the place .\n"
     ]
    }
   ],
   "source": [
    "# You can re-assemble this to see what the parser is going to see\n",
    "reassembled = [tagged[0] for tagged in mdg_tagged[0:51]]\n",
    "print(\" \".join(reassembled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the code block above is to demonstrate that the tagged document is simply a list of tuples, which you can manipulate a number of ways, here I am simply grabbing the first value in the tuple, `tuple[0]`. \n",
    "\n",
    "We could also do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large\n",
      "rainsford\n",
      "old\n",
      "ship\n",
      "trap\n",
      "suggestive\n",
      "curious\n",
      "i\n",
      "dank\n",
      "tropical\n",
      "palpable\n",
      "thick\n",
      "warm\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "for t in mdg_tagged[0:100]:\n",
    "    if t[1] == 'JJ':\n",
    "        print(t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Uh oh!*** NLTK isn't infallible! \n",
    "\n",
    "The NLTK has tagged \"rainsford\" and \"i\" as adjectives. (Tagging the \"I\" as an adjective is something I have encountered before.) First, you should know there are alternatives to NLTK, spaCy and TextBlob, but you also have to determine if these are one-off errors that you can ignore or if you can ignore these errors because of the scale of your work. This is an analytical judgment call: you must make it for yourself, but you must **document** it for others.\n",
    "\n",
    "*Documentation makes **you** look smart!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['large island', 'old charts', 'ship trap island whitney', 'suggestive name isn t', 'curious dread', 'i don t', 'dank tropical night', 'warm blackness', 've good eyes', 'i ve']\n"
     ]
    }
   ],
   "source": [
    "# Get the functionality we need\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create the blob\n",
    "blob = TextBlob(re.sub('[^a-zA-Z \\.]', ' ', mdg).lower())\n",
    "\n",
    "# Get the noun phrases (so easy!)\n",
    "mdg_np = blob.noun_phrases\n",
    "\n",
    "# See some results\n",
    "print(mdg_np[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Lions, tigers, and bear! Oh my!*** TextBlob produces weird results as well. This is not easy! (Think about the implications for large language models and how many inaccurate results are being \"averaged\" out.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('off', 'RB')\n",
      "('there', 'EX')\n",
      "('to', 'TO')\n",
      "(NP the/DT right/NN)\n",
      "('somewhere', 'RB')\n",
      "('is', 'VBZ')\n",
      "(NP a/DT large/JJ island/NN)\n",
      "('said', 'VBD')\n",
      "(NP whitney/NN)\n",
      "('.', '.')\n",
      "('it', 'PRP')\n",
      "('s', 'VBZ')\n",
      "('rather', 'RB')\n",
      "(NP a/DT mystery/NN)\n",
      "('what', 'WP')\n",
      "(NP island/NN)\n",
      "('is', 'VBZ')\n",
      "('it', 'PRP')\n",
      "('rainsford', 'JJ')\n",
      "('asked', 'VBD')\n"
     ]
    }
   ],
   "source": [
    "# How to Read the Grammar below:\n",
    "# Start with an optional (?) determiner ('DT')\n",
    "# Can have any number (*) of adjectives (JJ)\n",
    "# End with a noun (<NN>)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# Instantiate the chunk parser\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "# Run it on our tagged text\n",
    "tree = parser.parse(mdg_tagged)\n",
    "\n",
    "# See some results\n",
    "for i in tree[0:20]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP a/DT large/JJ island/NN)\n"
     ]
    }
   ],
   "source": [
    "print(tree[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7722 3\n"
     ]
    }
   ],
   "source": [
    "print(len(tree), tree.height())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(), (0,), (1,), (2,), (3,), (3, 0), (3, 1), (4,), (5,), (6,), (6, 0), (6, 1), (6, 2), (7,), (8,), (8, 0), (9,), (10,), (11,), (12,)]\n"
     ]
    }
   ],
   "source": [
    "print(tree.treepositions()[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593\n",
      "[Tree('NP', [('the', 'DT'), ('right', 'NN')]), Tree('NP', [('a', 'DT'), ('large', 'JJ'), ('island', 'NN')]), Tree('NP', [('whitney', 'NN')]), Tree('NP', [('a', 'DT'), ('mystery', 'NN')]), Tree('NP', [('island', 'NN')]), Tree('NP', [('ship', 'JJ'), ('trap', 'JJ'), ('island', 'NN')]), Tree('NP', [('whitney', 'NN')]), Tree('NP', [('a', 'DT'), ('suggestive', 'JJ'), ('name', 'NN')]), Tree('NP', [('isn', 'NN')]), Tree('NP', [('a', 'DT'), ('curious', 'JJ'), ('dread', 'NN')]), Tree('NP', [('the', 'DT'), ('place', 'NN')]), Tree('NP', [('some', 'DT'), ('superstition', 'NN')]), Tree('NP', [('rainsford', 'NN')]), Tree('NP', [('the', 'DT'), ('dank', 'JJ'), ('tropical', 'JJ'), ('night', 'NN')]), Tree('NP', [('thick', 'JJ'), ('warm', 'JJ'), ('blackness', 'NN')]), Tree('NP', [('the', 'DT'), ('yacht', 'NN')]), Tree('NP', [('whitney', 'NN')]), Tree('NP', [('a', 'DT'), ('laugh', 'NN')]), Tree('NP', [('i', 'NN')]), Tree('NP', [('a', 'DT'), ('moose', 'NN')])]\n"
     ]
    }
   ],
   "source": [
    "NPtrees = [subtree for subtree in tree if type(subtree) == nltk.Tree and subtree.label() == \"NP\"]\n",
    "\n",
    "print(len(NPtrees))\n",
    "print(NPtrees[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593\n",
      "[[('the', 'DT'), ('right', 'NN')], [('a', 'DT'), ('large', 'JJ'), ('island', 'NN')], [('whitney', 'NN')], [('a', 'DT'), ('mystery', 'NN')], [('island', 'NN')], [('ship', 'JJ'), ('trap', 'JJ'), ('island', 'NN')], [('whitney', 'NN')], [('a', 'DT'), ('suggestive', 'JJ'), ('name', 'NN')], [('isn', 'NN')], [('a', 'DT'), ('curious', 'JJ'), ('dread', 'NN')], [('the', 'DT'), ('place', 'NN')], [('some', 'DT'), ('superstition', 'NN')], [('rainsford', 'NN')], [('the', 'DT'), ('dank', 'JJ'), ('tropical', 'JJ'), ('night', 'NN')], [('thick', 'JJ'), ('warm', 'JJ'), ('blackness', 'NN')], [('the', 'DT'), ('yacht', 'NN')], [('whitney', 'NN')], [('a', 'DT'), ('laugh', 'NN')], [('i', 'NN')], [('a', 'DT'), ('moose', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "NPleaves = [subtree.leaves() for subtree in tree if type(subtree) == nltk.Tree and subtree.label() == \"NP\"]\n",
    "\n",
    "print(len(NPleaves))\n",
    "print(NPleaves[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the right\n",
      "a large island\n",
      "whitney\n",
      "a mystery\n",
      "island\n",
      "ship trap island\n",
      "whitney\n",
      "a suggestive name\n",
      "isn\n",
      "a curious dread\n",
      "the place\n",
      "some superstition\n",
      "rainsford\n",
      "the dank tropical night\n",
      "thick warm blackness\n",
      "the yacht\n",
      "whitney\n",
      "a laugh\n",
      "i\n",
      "a moose\n"
     ]
    }
   ],
   "source": [
    "for a_list in NPleaves[0:20]:\n",
    "    np = [word for word,tag in a_list]\n",
    "    phrase = \" \".join(np)\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grammar_hunt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parser \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mChartParser(\u001b[43mgrammar_hunt\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grammar_hunt' is not defined"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(grammar_hunt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trees = parser.parse(mdg_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities\n",
    "\n",
    "You will need to download the named entity chunker first: `nltk.download(\"maxent_ne_chunker\")`.\n",
    "\n",
    "For more on the kinds of named entities: https://www.nltk.org/book/ch07.html#sec-ner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = nltk.ne_chunk(mdg_tagged, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree\n",
    "# ModuleNotFoundError: No module named 'svgling'\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ne(text):\n",
    "    # tokenize by word\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    # apply part of speech tags to those words\n",
    "    tags = nltk.pos_tag(words)\n",
    "    # extract named entities based on those tags\n",
    "    # \"binary=True ==> named entities won’t be labeled by kind\n",
    "    tree = nltk.ne_chunk(tags, binary=True)\n",
    "    ne_set = set(\n",
    "        \" \".join(i[0] for i in t)\n",
    "        for t in tree\n",
    "        if hasattr(t, \"label\") and t.label() == \"NE\"\n",
    "    )\n",
    "    return ne_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Amazon', 'Rio', 'America', 'Rest', 'New York', 'Puritan', 'English', 'Caucasus', 'Island', 'American', 'Great White Czar', 'Uganda', 'Turkish', 'Malay', 'Rainsford', 'Suzette', 'Lazarus', 'Moscow', 'Russia', 'Africa', 'Ganges', 'General Zaroff', 'Malacca', 'Tibet', 'Purdey', 'Marcus Aurelius', 'Dusk', 'Caribbean', 'Mr. Rainsford', 'Caribbean Sea', 'Sleep', 'San Lucar', 'East Africa', 'God', 'Spanish', 'Pol Roger', 'Ship Trap', 'Sanger Rainsford', 'Chambertin', 'Chinese', 'Monte Carlo', 'Night', 'Crimea', 'Ivan', 'Madame Butterfly', 'Zaroff', 'Captain Nielsen', 'Toward', 'France', 'New York City', 'London', 'Rains', 'Whitney', 'Mr. Sanger Rainsford', 'French', 'Chablis', 'Veuve Cliquot', 'Russian', 'Great', 'Cossack', 'Paris'}\n"
     ]
    }
   ],
   "source": [
    "mdg_ne = extract_ne(mdg)\n",
    "print(mdg_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
