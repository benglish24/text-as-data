{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-4 Custom Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore how to take \"standard\" sentiments found in the NLTK and use them to tease out \"hidden\" sentiments in the gab corpus. This notebook uses the Real Python's [Sentiment Analysis: First Steps With Python's NLTK Library](https://realpython.com/python-nltk-sentiment-analysis/#using-nltks-pre-trained-sentiment-analyzer) as a starting place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the top line of the imports contains 3 modules. This is not considered \"pythonic\" by some, but it is acceptable. (That is, Python is not going to throw an error.) I don't tend to do this a lot, but sometimes when it's for well-known libraries, I just throw them all on the same line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import re, numpy as np, random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPL IMPORT & SETTINGS\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.dpi'] = 300\n",
    "# plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# I only need to do this once, so I put it in a cell all its own\n",
    "# and then comment it out once it's done so if/when I re-run this notebook,\n",
    "# I do not re-download the lexicon.\n",
    "\n",
    "# nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70596\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "# Load the gabs:\n",
    "with open(\"../queue/gabs.txt\", \"r\") as f:\n",
    "    gabs = f.readlines()\n",
    "\n",
    "# How many do we have?\n",
    "print(len(gabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some point in working through this notebook, I realized that there were enough posts that were simply links that I wanted to remove them. It may well be that attending to what people linked to in Gab will be something to look into later, but for now I want to focus only on what people write.\n",
    "\n",
    "The code below is supposed to remove any links it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = re.compile(r'<[^>]*>')\n",
    "html_free = [ re.sub(html, \" \", gab) for gab in gabs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>NOTE:</b> This simple change shifted the numbers in the section below rather significantly. It had a large impact on the longest texts, perhaps suggesting that longer texts were made up of lots of links.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a Better Grip on the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I want to try something a little different with the gab data: I am not interested in status checks or short posts. I am only really interested, in general, in the longer posts. In the cells that follow I want to explore the lengths of the gabs to determine if there is a more a threshold, a floor in particular,  that I can use to exlcude statuses, quick back and forths, and link and runs.  \n",
    "\n",
    "My first impulse was to do a simple character count, and I may yet do that, but I think I will try to be more \"textual\" and start with a word count. \n",
    "\n",
    "I'll start with some simple calculations and then try a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is in a cell by itself because tokenizing takes time, \n",
    "# and we only want to do it once.\n",
    "tokenized = []\n",
    "for i in html_free:\n",
    "    tokens = word_tokenize(i)\n",
    "    tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is an example of a list comprehension -- the code inside the square brackets, [] -- embedded inside two functions, first a length and then a print. It's simple, compact, and it allows me to change the number at the end and re-run the cell to \"map\" out the data a bit in my mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18458\n",
      "35309\n"
     ]
    }
   ],
   "source": [
    "print(len([i for i in tokenized if len(i) < 5]))\n",
    "print(len([i for i in tokenized if len(i) < 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two simple lines reveal that almost half the gabs are less than 10 words and about a quarter are less than five words.\n",
    "\n",
    "If we want a somewhat more nuanced \"mapping\" of the corpus, we can use numpy's `histogram`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62486  5779  1310   462   225   114    71    61    65    23]\n",
      "[  0.   38.7  77.4 116.1 154.8 193.5 232.2 270.9 309.6 348.3 387. ]\n"
     ]
    }
   ],
   "source": [
    "lengths = [ len(i) for i in tokenized ]\n",
    "counts, bins = np.histogram(lengths)\n",
    "print(counts)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histogram's default number of bins is 10, and so it divides the longest post by 10 and creates a bin for each tenth of that size. Most of the posts are less than 300 words in length.\n",
    "\n",
    "While it might be worth looking at the 4 posts that weigh in at 1500-3000 words, I am really intrigued by the 138 posts that are 300-600 words long. I am bookmarking those as ones to return to.\n",
    "\n",
    "**Conclusions**: dropping posts less than five words is pretty straightforward. The real question is: how interesting are the posts that are 5-10 words long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorts = [post for post in tokenized if len(post) > 5 and len(post) < 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viva Cristo Rey , brother .\n",
      "My Q day was awesome , Thank You ❤️❤️\n",
      "Kunde inte klicka på följ , fanns ingen sådan .\n",
      "Hi , how are you today ?\n",
      "Lets get keep maga movement going Patriots_Unite\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(shorts, 5):\n",
    "    print(\" \".join(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running that cell 3-5 times, I did not see any gabs that looked terribly interesting, so I am going to set my floor at 10 words, eliminating half my corpus from consideration. *Sigh*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIRMATION OF REGISTRATION what caption do we see in site header : Tricky question\n"
     ]
    }
   ],
   "source": [
    "# List comprehension for gabs of greater than 10 words\n",
    "texts = [ post for post in tokenized if len(post) > 10 ]\n",
    "\n",
    "# Join the gabs back together because NLTK's sentiment expects it?\n",
    "joins = [ \" \".join(text) for text in texts ]\n",
    "\n",
    "# Let's see a random one of them:\n",
    "print(random.choice(joins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment the NLTK Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.543, 'pos': 0.457, 'compound': 0.6369}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to get a score\n",
    "sia.polarity_scores(\"Python is the best programming language.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoping this day greets you with good health and peace of mind .\n",
      "{'neg': 0.0, 'neu': 0.426, 'pos': 0.574, 'compound': 0.8689}\n"
     ]
    }
   ],
   "source": [
    "# What it looks like for our corpus:\n",
    "sample = random.choice(joins)\n",
    "print(sample)\n",
    "print(sia.polarity_scores(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there ! I 'm just wasting some time at work right now haha . How are you doing ?\n",
      "0.1511\n"
     ]
    }
   ],
   "source": [
    "# And now just getting the compound:\n",
    "sample = random.choice(joins)\n",
    "print(sample)\n",
    "print(sia.polarity_scores(sample)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
